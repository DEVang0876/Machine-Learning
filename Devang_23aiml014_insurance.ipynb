{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "832c35d8",
   "metadata": {},
   "source": [
    "# Health Insurance Premium Prediction Project\n",
    "\n",
    "## Project Overview\n",
    "This project aims to predict health insurance premiums using Ordinary Least Squares (OLS) regression with comprehensive feature engineering and selection techniques including:\n",
    "- Variance Inflation Factor (VIF) analysis for multicollinearity detection\n",
    "- Correlation analysis for feature selection\n",
    "- Feature engineering to improve model performance\n",
    "\n",
    "## Dataset\n",
    "The dataset contains health insurance information with features like age, sex, BMI, children, smoker status, region, and insurance charges.\n",
    "\n",
    "## Methodology\n",
    "1. **Data Exploration and Preprocessing**\n",
    "2. **Feature Engineering**\n",
    "3. **Feature Selection using Correlation and VIF**\n",
    "4. **OLS Model Implementation**\n",
    "5. **Model Evaluation and Diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f18c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation, visualization, and modeling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6efc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the insurance dataset\n",
    "df = pd.read_csv('insurance.csv')\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b11966",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80134833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in each column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Check for duplicate records\n",
    "print(f\"\\nNumber of duplicate records: {df.duplicated().sum()}\")\n",
    "\n",
    "# Display unique values for categorical columns\n",
    "categorical_cols = ['sex', 'smoker', 'region']\n",
    "for col in categorical_cols:\n",
    "    print(f\"\\nUnique values in {col}: {df[col].unique()}\")\n",
    "    print(f\"Value counts for {col}:\")\n",
    "    print(df[col].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7899533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for data distribution\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Distribution of numerical variables\n",
    "numerical_cols = ['age', 'bmi', 'children', 'charges']\n",
    "for i, col in enumerate(numerical_cols):\n",
    "    row = i // 2\n",
    "    col_idx = i % 2\n",
    "    axes[row, col_idx].hist(df[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "    axes[row, col_idx].set_title(f'Distribution of {col}')\n",
    "    axes[row, col_idx].set_xlabel(col)\n",
    "    axes[row, col_idx].set_ylabel('Frequency')\n",
    "\n",
    "# Bar plots for categorical variables\n",
    "axes[0, 2].bar(df['sex'].value_counts().index, df['sex'].value_counts().values)\n",
    "axes[0, 2].set_title('Distribution of Sex')\n",
    "axes[0, 2].set_xlabel('Sex')\n",
    "axes[0, 2].set_ylabel('Count')\n",
    "\n",
    "axes[1, 2].bar(df['smoker'].value_counts().index, df['smoker'].value_counts().values)\n",
    "axes[1, 2].set_title('Distribution of Smoker')\n",
    "axes[1, 2].set_xlabel('Smoker')\n",
    "axes[1, 2].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the target variable (charges) in more detail\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['charges'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Insurance Charges')\n",
    "plt.xlabel('Charges')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(np.log(df['charges']), bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Log(Charges)')\n",
    "plt.xlabel('Log(Charges)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.boxplot(df['charges'])\n",
    "plt.title('Boxplot of Insurance Charges')\n",
    "plt.ylabel('Charges')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical summary of charges\n",
    "print(\"Statistical Summary of Insurance Charges:\")\n",
    "print(f\"Mean: ${df['charges'].mean():.2f}\")\n",
    "print(f\"Median: ${df['charges'].median():.2f}\")\n",
    "print(f\"Standard Deviation: ${df['charges'].std():.2f}\")\n",
    "print(f\"Minimum: ${df['charges'].min():.2f}\")\n",
    "print(f\"Maximum: ${df['charges'].max():.2f}\")\n",
    "print(f\"Skewness: {df['charges'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df['charges'].kurtosis():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a299a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze relationships between features and target variable\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Age vs Charges\n",
    "axes[0, 0].scatter(df['age'], df['charges'], alpha=0.6)\n",
    "axes[0, 0].set_title('Age vs Insurance Charges')\n",
    "axes[0, 0].set_xlabel('Age')\n",
    "axes[0, 0].set_ylabel('Charges')\n",
    "\n",
    "# BMI vs Charges\n",
    "axes[0, 1].scatter(df['bmi'], df['charges'], alpha=0.6)\n",
    "axes[0, 1].set_title('BMI vs Insurance Charges')\n",
    "axes[0, 1].set_xlabel('BMI')\n",
    "axes[0, 1].set_ylabel('Charges')\n",
    "\n",
    "# Children vs Charges\n",
    "axes[0, 2].boxplot([df[df['children'] == i]['charges'].values for i in range(6)], \n",
    "                   labels=range(6))\n",
    "axes[0, 2].set_title('Children vs Insurance Charges')\n",
    "axes[0, 2].set_xlabel('Number of Children')\n",
    "axes[0, 2].set_ylabel('Charges')\n",
    "\n",
    "# Sex vs Charges\n",
    "sns.boxplot(data=df, x='sex', y='charges', ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Sex vs Insurance Charges')\n",
    "\n",
    "# Smoker vs Charges\n",
    "sns.boxplot(data=df, x='smoker', y='charges', ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Smoker vs Insurance Charges')\n",
    "\n",
    "# Region vs Charges\n",
    "sns.boxplot(data=df, x='region', y='charges', ax=axes[1, 2])\n",
    "axes[1, 2].set_title('Region vs Insurance Charges')\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf03cd8",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15a3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the dataset for feature engineering\n",
    "df_engineered = df.copy()\n",
    "\n",
    "# 1. Create BMI categories\n",
    "def categorize_bmi(bmi):\n",
    "    if bmi < 18.5:\n",
    "        return 'Underweight'\n",
    "    elif bmi < 25:\n",
    "        return 'Normal'\n",
    "    elif bmi < 30:\n",
    "        return 'Overweight'\n",
    "    else:\n",
    "        return 'Obese'\n",
    "\n",
    "df_engineered['bmi_category'] = df_engineered['bmi'].apply(categorize_bmi)\n",
    "\n",
    "# 2. Create age groups\n",
    "def categorize_age(age):\n",
    "    if age < 25:\n",
    "        return 'Young'\n",
    "    elif age < 35:\n",
    "        return 'Young Adult'\n",
    "    elif age < 50:\n",
    "        return 'Middle Age'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "\n",
    "df_engineered['age_group'] = df_engineered['age'].apply(categorize_age)\n",
    "\n",
    "# 3. Create interaction features\n",
    "df_engineered['age_bmi_interaction'] = df_engineered['age'] * df_engineered['bmi']\n",
    "df_engineered['smoker_age_interaction'] = df_engineered['age'] * (df_engineered['smoker'] == 'yes').astype(int)\n",
    "df_engineered['smoker_bmi_interaction'] = df_engineered['bmi'] * (df_engineered['smoker'] == 'yes').astype(int)\n",
    "\n",
    "# 4. Create polynomial features\n",
    "df_engineered['age_squared'] = df_engineered['age'] ** 2\n",
    "df_engineered['bmi_squared'] = df_engineered['bmi'] ** 2\n",
    "\n",
    "# 5. Create binary features for high-risk categories\n",
    "df_engineered['high_bmi'] = (df_engineered['bmi'] >= 30).astype(int)\n",
    "df_engineered['senior_citizen'] = (df_engineered['age'] >= 50).astype(int)\n",
    "\n",
    "print(\"New features created:\")\n",
    "new_features = ['bmi_category', 'age_group', 'age_bmi_interaction', 'smoker_age_interaction', \n",
    "                'smoker_bmi_interaction', 'age_squared', 'bmi_squared', 'high_bmi', 'senior_citizen']\n",
    "for feature in new_features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "print(f\"\\nDataset shape after feature engineering: {df_engineered.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d04e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables using Label Encoding and One-Hot Encoding\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label encode binary categorical variables\n",
    "le_sex = LabelEncoder()\n",
    "le_smoker = LabelEncoder()\n",
    "\n",
    "df_engineered['sex_encoded'] = le_sex.fit_transform(df_engineered['sex'])\n",
    "df_engineered['smoker_encoded'] = le_smoker.fit_transform(df_engineered['smoker'])\n",
    "\n",
    "# One-hot encode multi-class categorical variables\n",
    "region_dummies = pd.get_dummies(df_engineered['region'], prefix='region')\n",
    "bmi_category_dummies = pd.get_dummies(df_engineered['bmi_category'], prefix='bmi_cat')\n",
    "age_group_dummies = pd.get_dummies(df_engineered['age_group'], prefix='age_grp')\n",
    "\n",
    "# Combine all features\n",
    "df_final = pd.concat([df_engineered, region_dummies, bmi_category_dummies, age_group_dummies], axis=1)\n",
    "\n",
    "# Drop original categorical columns\n",
    "columns_to_drop = ['sex', 'smoker', 'region', 'bmi_category', 'age_group']\n",
    "df_final = df_final.drop(columns=columns_to_drop)\n",
    "\n",
    "print(\"Categorical encoding completed!\")\n",
    "print(f\"Final dataset shape: {df_final.shape}\")\n",
    "print(\"\\nFinal features:\")\n",
    "print(df_final.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e502a9",
   "metadata": {},
   "source": [
    "## Feature Selection using Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d498854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_final.corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(16, 12))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'shrink': 0.5}, mask=mask)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify highly correlated features (correlation > 0.8 or < -0.8)\n",
    "def find_high_correlation_pairs(corr_matrix, threshold=0.8):\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i+1, len(corr_matrix.columns)):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "                high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n",
    "    return high_corr_pairs\n",
    "\n",
    "high_corr_pairs = find_high_correlation_pairs(correlation_matrix, threshold=0.8)\n",
    "print(\"Highly correlated feature pairs (|correlation| > 0.8):\")\n",
    "for pair in high_corr_pairs:\n",
    "    print(f\"{pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "# Correlation with target variable\n",
    "target_correlation = correlation_matrix['charges'].sort_values(key=abs, ascending=False)\n",
    "print(\"\\nCorrelation with target variable (charges):\")\n",
    "print(target_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82397c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated features based on correlation analysis\n",
    "# We'll remove one feature from each highly correlated pair, keeping the one more correlated with target\n",
    "\n",
    "features_to_remove = set()\n",
    "\n",
    "# Remove features with very low correlation with target (|correlation| < 0.05)\n",
    "low_correlation_features = target_correlation[abs(target_correlation) < 0.05].index.tolist()\n",
    "if 'charges' in low_correlation_features:\n",
    "    low_correlation_features.remove('charges')\n",
    "\n",
    "print(\"Features with low correlation to target (|correlation| < 0.05):\")\n",
    "print(low_correlation_features)\n",
    "\n",
    "# For highly correlated pairs, remove the one with lower target correlation\n",
    "for pair in high_corr_pairs:\n",
    "    feature1, feature2 = pair[0], pair[1]\n",
    "    if feature1 != 'charges' and feature2 != 'charges':\n",
    "        corr1 = abs(target_correlation[feature1])\n",
    "        corr2 = abs(target_correlation[feature2])\n",
    "        if corr1 < corr2:\n",
    "            features_to_remove.add(feature1)\n",
    "        else:\n",
    "            features_to_remove.add(feature2)\n",
    "\n",
    "# Combine all features to remove\n",
    "features_to_remove.update(low_correlation_features)\n",
    "features_to_remove = list(features_to_remove)\n",
    "\n",
    "print(f\"\\nFeatures to remove based on correlation analysis: {features_to_remove}\")\n",
    "\n",
    "# Create dataset after correlation-based feature removal\n",
    "df_corr_selected = df_final.drop(columns=features_to_remove)\n",
    "print(f\"\\nDataset shape after correlation-based selection: {df_corr_selected.shape}\")\n",
    "print(\"Remaining features:\")\n",
    "print([col for col in df_corr_selected.columns if col != 'charges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e85412",
   "metadata": {},
   "source": [
    "## VIF Analysis and Multicollinearity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7690b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VIF (Variance Inflation Factor) for multicollinearity detection\n",
    "def calculate_vif(df, target_col='charges'):\n",
    "    \"\"\"Calculate VIF for all features except target variable\"\"\"\n",
    "    X = df.drop(columns=[target_col])\n",
    "    \n",
    "    # Calculate VIF for each feature\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"Feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "    \n",
    "    return vif_data.sort_values('VIF', ascending=False)\n",
    "\n",
    "# Calculate VIF for correlation-selected features\n",
    "print(\"VIF Analysis after correlation-based feature selection:\")\n",
    "vif_results = calculate_vif(df_corr_selected)\n",
    "print(vif_results)\n",
    "\n",
    "# Visualize VIF values\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red' if vif > 10 else 'orange' if vif > 5 else 'green' for vif in vif_results['VIF']]\n",
    "bars = plt.barh(range(len(vif_results)), vif_results['VIF'], color=colors)\n",
    "plt.yticks(range(len(vif_results)), vif_results['Feature'])\n",
    "plt.xlabel('VIF Value')\n",
    "plt.title('Variance Inflation Factor (VIF) for Each Feature')\n",
    "plt.axvline(x=5, color='orange', linestyle='--', alpha=0.7, label='VIF = 5 (Warning)')\n",
    "plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='VIF = 10 (High Multicollinearity)')\n",
    "plt.legend()\n",
    "\n",
    "# Add VIF values on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    plt.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, \n",
    "             f'{vif_results.iloc[i][\"VIF\"]:.1f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify features with high VIF (> 10)\n",
    "high_vif_features = vif_results[vif_results['VIF'] > 10]['Feature'].tolist()\n",
    "print(f\"\\nFeatures with high VIF (> 10): {high_vif_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feece406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features with high VIF iteratively\n",
    "def remove_high_vif_features(df, target_col='charges', threshold=10):\n",
    "    \"\"\"Remove features with VIF > threshold iteratively\"\"\"\n",
    "    df_vif = df.copy()\n",
    "    removed_features = []\n",
    "    \n",
    "    while True:\n",
    "        vif_data = calculate_vif(df_vif, target_col)\n",
    "        max_vif = vif_data['VIF'].max()\n",
    "        \n",
    "        if max_vif <= threshold:\n",
    "            break\n",
    "            \n",
    "        # Remove feature with highest VIF\n",
    "        feature_to_remove = vif_data[vif_data['VIF'] == max_vif]['Feature'].iloc[0]\n",
    "        removed_features.append(feature_to_remove)\n",
    "        df_vif = df_vif.drop(columns=[feature_to_remove])\n",
    "        \n",
    "        print(f\"Removed {feature_to_remove} (VIF: {max_vif:.2f})\")\n",
    "    \n",
    "    return df_vif, removed_features\n",
    "\n",
    "# Apply VIF-based feature removal\n",
    "print(\"Iteratively removing features with high VIF...\")\n",
    "df_final_selected, removed_vif_features = remove_high_vif_features(df_corr_selected, threshold=10)\n",
    "\n",
    "print(f\"\\nFeatures removed due to high VIF: {removed_vif_features}\")\n",
    "print(f\"Final dataset shape: {df_final_selected.shape}\")\n",
    "\n",
    "# Calculate final VIF values\n",
    "print(\"\\nFinal VIF values:\")\n",
    "final_vif = calculate_vif(df_final_selected)\n",
    "print(final_vif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda2833",
   "metadata": {},
   "source": [
    "## OLS Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaaa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for OLS modeling\n",
    "X = df_final_selected.drop('charges', axis=1)\n",
    "y = df_final_selected['charges']\n",
    "\n",
    "print(\"Final features for modeling:\")\n",
    "print(X.columns.tolist())\n",
    "print(f\"Number of features: {len(X.columns)}\")\n",
    "print(f\"Number of samples: {len(X)}\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Scale features for better model performance\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for better interpretation\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d09bf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build OLS model using statsmodels for detailed statistical analysis\n",
    "X_train_with_const = sm.add_constant(X_train_scaled)\n",
    "X_test_with_const = sm.add_constant(X_test_scaled)\n",
    "\n",
    "# Fit OLS model\n",
    "ols_model = sm.OLS(y_train, X_train_with_const).fit()\n",
    "\n",
    "# Print detailed model summary\n",
    "print(\"OLS Model Summary:\")\n",
    "print(\"=\" * 80)\n",
    "print(ols_model.summary())\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = ols_model.predict(X_train_with_const)\n",
    "y_test_pred = ols_model.predict(X_test_with_const)\n",
    "\n",
    "# Calculate performance metrics\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL PERFORMANCE METRICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training R²: {train_r2:.4f}\")\n",
    "print(f\"Testing R²: {test_r2:.4f}\")\n",
    "print(f\"Training RMSE: ${train_rmse:.2f}\")\n",
    "print(f\"Testing RMSE: ${test_rmse:.2f}\")\n",
    "print(f\"Training MAE: ${train_mae:.2f}\")\n",
    "print(f\"Testing MAE: ${test_mae:.2f}\")\n",
    "\n",
    "# Calculate adjusted R-squared\n",
    "n = len(y_train)\n",
    "p = len(X_train.columns)\n",
    "adj_r2_train = 1 - (1 - train_r2) * (n - 1) / (n - p - 1)\n",
    "print(f\"Adjusted R² (Training): {adj_r2_train:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67d191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': ols_model.params[1:],  # Exclude constant\n",
    "    'P-value': ols_model.pvalues[1:],     # Exclude constant\n",
    "    'Std_Error': ols_model.bse[1:],       # Exclude constant\n",
    "    'Abs_Coefficient': np.abs(ols_model.params[1:])\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['green' if p < 0.05 else 'red' for p in feature_importance['P-value']]\n",
    "bars = plt.barh(range(len(feature_importance)), feature_importance['Abs_Coefficient'], color=colors)\n",
    "plt.yticks(range(len(feature_importance)), feature_importance['Feature'])\n",
    "plt.xlabel('Absolute Coefficient Value')\n",
    "plt.title('Feature Importance (Absolute Coefficient Values)\\nGreen: Significant (p<0.05), Red: Not Significant')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add coefficient values on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    coef_val = feature_importance.iloc[i]['Coefficient']\n",
    "    plt.text(bar.get_width() + 0.01 * max(feature_importance['Abs_Coefficient']), \n",
    "             bar.get_y() + bar.get_height()/2, \n",
    "             f'{coef_val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count significant features\n",
    "significant_features = feature_importance[feature_importance['P-value'] < 0.05]\n",
    "print(f\"\\nNumber of statistically significant features (p < 0.05): {len(significant_features)}\")\n",
    "print(\"Significant features:\")\n",
    "print(significant_features[['Feature', 'Coefficient', 'P-value']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bece2a",
   "metadata": {},
   "source": [
    "## Model Evaluation and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ce7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model diagnostic plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# 1. Actual vs Predicted (Training)\n",
    "axes[0, 0].scatter(y_train, y_train_pred, alpha=0.6, color='blue')\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Charges')\n",
    "axes[0, 0].set_ylabel('Predicted Charges')\n",
    "axes[0, 0].set_title(f'Training: Actual vs Predicted\\nR² = {train_r2:.3f}')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Actual vs Predicted (Testing)\n",
    "axes[0, 1].scatter(y_test, y_test_pred, alpha=0.6, color='green')\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Charges')\n",
    "axes[0, 1].set_ylabel('Predicted Charges')\n",
    "axes[0, 1].set_title(f'Testing: Actual vs Predicted\\nR² = {test_r2:.3f}')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Residuals vs Predicted (Training)\n",
    "train_residuals = y_train - y_train_pred\n",
    "axes[0, 2].scatter(y_train_pred, train_residuals, alpha=0.6, color='blue')\n",
    "axes[0, 2].axhline(y=0, color='r', linestyle='--')\n",
    "axes[0, 2].set_xlabel('Predicted Charges')\n",
    "axes[0, 2].set_ylabel('Residuals')\n",
    "axes[0, 2].set_title('Training: Residuals vs Predicted')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Residuals vs Predicted (Testing)\n",
    "test_residuals = y_test - y_test_pred\n",
    "axes[1, 0].scatter(y_test_pred, test_residuals, alpha=0.6, color='green')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Predicted Charges')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Testing: Residuals vs Predicted')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Q-Q Plot for residuals normality check\n",
    "from scipy import stats\n",
    "stats.probplot(train_residuals, dist=\"norm\", plot=axes[1, 1])\n",
    "axes[1, 1].set_title('Q-Q Plot: Residuals Normality Check')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Distribution of residuals\n",
    "axes[1, 2].hist(train_residuals, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[1, 2].set_xlabel('Residuals')\n",
    "axes[1, 2].set_ylabel('Frequency')\n",
    "axes[1, 2].set_title('Distribution of Residuals')\n",
    "axes[1, 2].axvline(x=0, color='r', linestyle='--')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical tests for model assumptions\n",
    "from scipy.stats import shapiro, jarque_bera\n",
    "\n",
    "print(\"STATISTICAL TESTS FOR MODEL ASSUMPTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Test for normality of residuals\n",
    "shapiro_stat, shapiro_p = shapiro(train_residuals)\n",
    "jb_stat, jb_p = jarque_bera(train_residuals)\n",
    "\n",
    "print(\"1. NORMALITY OF RESIDUALS:\")\n",
    "print(f\"   Shapiro-Wilk Test: statistic = {shapiro_stat:.4f}, p-value = {shapiro_p:.4f}\")\n",
    "print(f\"   Jarque-Bera Test: statistic = {jb_stat:.4f}, p-value = {jb_p:.4f}\")\n",
    "if shapiro_p > 0.05:\n",
    "    print(\"   → Residuals appear to be normally distributed (p > 0.05)\")\n",
    "else:\n",
    "    print(\"   → Residuals may not be normally distributed (p ≤ 0.05)\")\n",
    "\n",
    "# 2. Test for homoscedasticity (constant variance)\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "\n",
    "bp_stat, bp_p, f_stat, f_p = het_breuschpagan(train_residuals, X_train_with_const)\n",
    "print(f\"\\n2. HOMOSCEDASTICITY (CONSTANT VARIANCE):\")\n",
    "print(f\"   Breusch-Pagan Test: statistic = {bp_stat:.4f}, p-value = {bp_p:.4f}\")\n",
    "if bp_p > 0.05:\n",
    "    print(\"   → Homoscedasticity assumption satisfied (p > 0.05)\")\n",
    "else:\n",
    "    print(\"   → Heteroscedasticity detected (p ≤ 0.05)\")\n",
    "\n",
    "# 3. Durbin-Watson test for autocorrelation\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "dw_stat = durbin_watson(train_residuals)\n",
    "print(f\"\\n3. AUTOCORRELATION:\")\n",
    "print(f\"   Durbin-Watson Test: statistic = {dw_stat:.4f}\")\n",
    "if 1.5 < dw_stat < 2.5:\n",
    "    print(\"   → No significant autocorrelation (1.5 < DW < 2.5)\")\n",
    "else:\n",
    "    print(\"   → Potential autocorrelation detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1525231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison with different approaches\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Compare with a model using all original features (before feature selection)\n",
    "X_all = df_final.drop('charges', axis=1)\n",
    "X_all_train, X_all_test, _, _ = train_test_split(X_all, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale all features\n",
    "X_all_train_scaled = scaler.fit_transform(X_all_train)\n",
    "X_all_test_scaled = scaler.transform(X_all_test)\n",
    "\n",
    "# Fit model with all features\n",
    "X_all_train_with_const = sm.add_constant(X_all_train_scaled)\n",
    "X_all_test_with_const = sm.add_constant(X_all_test_scaled)\n",
    "\n",
    "try:\n",
    "    ols_all_features = sm.OLS(y_train, X_all_train_with_const).fit()\n",
    "    y_all_pred = ols_all_features.predict(X_all_test_with_const)\n",
    "    r2_all_features = r2_score(y_test, y_all_pred)\n",
    "    rmse_all_features = np.sqrt(mean_squared_error(y_test, y_all_pred))\n",
    "    \n",
    "    print(\"1. ALL FEATURES MODEL:\")\n",
    "    print(f\"   Number of features: {len(X_all.columns)}\")\n",
    "    print(f\"   R²: {r2_all_features:.4f}\")\n",
    "    print(f\"   RMSE: ${rmse_all_features:.2f}\")\n",
    "except:\n",
    "    print(\"1. ALL FEATURES MODEL: Failed to fit (likely due to multicollinearity)\")\n",
    "\n",
    "print(f\"\\n2. OPTIMIZED MODEL (AFTER FEATURE SELECTION):\")\n",
    "print(f\"   Number of features: {len(X.columns)}\")\n",
    "print(f\"   R²: {test_r2:.4f}\")\n",
    "print(f\"   RMSE: ${test_rmse:.2f}\")\n",
    "\n",
    "# 3. Simple baseline model (using only most correlated features)\n",
    "baseline_features = ['smoker_encoded', 'age', 'bmi']\n",
    "X_baseline = df_final[baseline_features]\n",
    "X_baseline_train, X_baseline_test, _, _ = train_test_split(X_baseline, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_baseline_train_scaled = scaler.fit_transform(X_baseline_train)\n",
    "X_baseline_test_scaled = scaler.transform(X_baseline_test)\n",
    "\n",
    "X_baseline_train_with_const = sm.add_constant(X_baseline_train_scaled)\n",
    "X_baseline_test_with_const = sm.add_constant(X_baseline_test_scaled)\n",
    "\n",
    "ols_baseline = sm.OLS(y_train, X_baseline_train_with_const).fit()\n",
    "y_baseline_pred = ols_baseline.predict(X_baseline_test_with_const)\n",
    "r2_baseline = r2_score(y_test, y_baseline_pred)\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_test, y_baseline_pred))\n",
    "\n",
    "print(f\"\\n3. BASELINE MODEL (3 MOST CORRELATED FEATURES):\")\n",
    "print(f\"   Features: {baseline_features}\")\n",
    "print(f\"   R²: {r2_baseline:.4f}\")\n",
    "print(f\"   RMSE: ${rmse_baseline:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac3d64c",
   "metadata": {},
   "source": [
    "## Conclusions and Insights\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Feature Engineering Impact**: Created meaningful features like BMI categories, age groups, and interaction terms that improved model interpretability and performance.\n",
    "\n",
    "2. **Feature Selection Effectiveness**: \n",
    "   - Correlation analysis helped identify redundant features\n",
    "   - VIF analysis successfully removed multicollinear features\n",
    "   - Final model uses only the most important, non-redundant features\n",
    "\n",
    "3. **Model Performance**:\n",
    "   - Achieved good predictive performance with R² indicating strong explanatory power\n",
    "   - Feature selection improved model stability and interpretability\n",
    "   - Statistical significance tests validate feature importance\n",
    "\n",
    "4. **Important Predictors**: The most significant predictors of insurance charges include smoking status, age, BMI, and their interactions.\n",
    "\n",
    "5. **Model Assumptions**: Diagnostic tests help validate the appropriateness of OLS regression for this dataset.\n",
    "\n",
    "### Business Insights:\n",
    "- Smoking status is the strongest predictor of insurance charges\n",
    "- Age and BMI have significant positive relationships with charges\n",
    "- Interaction effects between variables provide additional predictive power\n",
    "- The model can be used for pricing strategies and risk assessment"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
